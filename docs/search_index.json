[
["index.html", "VR Experience (ML-agents) ML-Agents Language License", " VR Experience (ML-agents) 34145/1928/2021/1/56David D’Haese Gepubliceerd op 2020-10-20Spirogyra sp., Motic BA310 ML-Agents In the course VR Experience the student is being submerged in VR- and 3D game development using the Unity engine. The student will learn to work with different camera perspectives, the physics engine, audio, light and scripting. In addition, using the ML-Agents toolkit, the student will learn to enrich applications with AI. The student will be able to train agents using artificial neural networks or other ML algorithms. This repository deal only with the latter part of the course, namely the the use of ML-agents as well as a basic introduction to ML. Language The course is in Dutch. License The content of this project including its source code is licensed under the GNU Affero General Public v3.0 license. You are allowed to… Under the conditions… You are not allowed to… Commercial use Disclose source Liability Distribution License and copyright notice Warranty Modification Network use is distribution Patent use Same license Private use State changes © 2020 David D’Haese "],
["inleiding-tot-het-onderdeel-ml-agents.html", "Hoofdstuk 1 Inleiding tot het onderdeel ML-Agents 1.1 In een notedop 1.2 Leerdoelen 1.3 Cursus vorm 1.4 Licentie voor deze cursus 1.5 Verwijzen naar deze cursus", " Hoofdstuk 1 Inleiding tot het onderdeel ML-Agents 1.1 In een notedop In dit opleidingsonderdeel ga je in teamverband aan de slag met de Unity game engine en leer je de 3D-toepassingen ontwikkelen. Deze 3D-toepassingen gaan van games tot interactieve Virtual Reality applicaties. Je leert werken met verschillende camera perspectieven, de physics engine, geluid, licht en scripts. Met ML-Agents Toolkit leer je hoe je met artificiële intelligentie een virtuele omgeving en zijn agents kunt laten aanpassen aan impulsen van buitenaf. Dit archief biedt hulp bij het gebruik van ML-agents en geeft ook een algemene introductie tot ML. 1.2 Leerdoelen Hieronder, in Tabel 1.1, staan de leerdoelen opgesomd zoals ze vermeld staan in de ECTS fiches die van toepassing zijn voor dit onderdeel van het OLOD VR Experience. In de cursus zal er naar deze leerdoelen verwezen worden met vermelding van de bijhorende code. Tabel 1.1: Leerdoelen voor deze cursus Code Omschrijving TI_LD568 Begrijpt de principes van artificiële intelligentie TI_LD569 Ontwikkelt een AI-module dat een cruciaal onderdeel vormt van een Unity project TI_LD570 Gebruikt en kent specifieke ML bibliotheken TI_LD571 Traint, test en evalueert een machine learning algoritme volgens de regels van de kunst 1.3 Cursus vorm Deze cursus is geschreven in een versie van Markdown. Markdown is een familie van zogenaamde opmaaktalen (eng: mark-up languages) die ervoor zorgen dat inhoud van het document in verscheidene formaten weergegeven kan worden: PDF, HTML, …. Het loskoppelen van inhoud betekent enerzijds dat de auteur zich kan focusseren op de inhoud in plaats van de vorm. Anderzijds betekent het dat de lezer in staat is zijn de vorm van de uitvoer te bepalen, bijvoorbeeld, beter leesbaarheid, donkere achtergrond, …. Voor meer technische documenten biedt Markdown nog veel belangrijkere voordelen. Het maakt het mogelijk om code in de ene of andere taal tussen de lyrische tekst te plaatsen en uit te voeren. Met de juiste IDE (Integrated Development Environment), betekent dit dat de auteur én de lezer in staat zijn om in meerdere programmeertalen tegelijkertijd te werken! Figuur 1.1: Werking van Markdown. De platte tekst (links) wordt omgezet naar een ander formaat (rechts; hier HTML) door een externe tool als Pandoc. Stijl-regels worden hier automatisch uitgevoerd maar de auteur heeft de mogelijkheid ook deze in detail te configureren. Naast het scheiden van vormgeving en inhoud (hetgeen een merkelijke efficiëntie verbetering met zich meebrengt) ondersteund R Markdown ook meertaligheid, i.e. meerdere programmeertalen in één document. Tussen een aantal talen is er zelfs wederzijdse uitwisseling mogelijk van de actieve variabelen (zie oranje markeringen met pijl). Het voorbeeld met de Mandelbrot fractaal is afkomstig van Li (2017), waarvan de GitHub repository een bondige beschrijving geeft van de Mandelbrot verzameling (eng: Mandelbrot set) met een test die de performantie-winst van Julia t.o.v. R onderzoekt. 1.4 Licentie voor deze cursus De inhoud van deze cursus valt onder een GNU Affero General Public v3.0 licentie. Wat er toegelaten is en onder welke voorwaarden staat hieronder opgesomd: Je mag… Onder voorwaarde dat… Je mag niet… Commercieel gebruik Ontsluit bron Aansprakelijk stellen Verspreiden Licentie en copyright notice mee verspreiden Garantie Aanpassen Netwerk verspreiding Patenteren Zelfde licentie Privé gebruiken Bekendmaking wijzigingen 1.5 Verwijzen naar deze cursus In Bibtex-formaat: @online{dhaese2020ml-agents, author = {D’Haese, David}, title = “VR Experience (ML-agents)”, year = “2020”, url = “https://ddhaese.github.io/vr-experience/”, note = \"[Online; accessed } APA-formaat: D’Haese, D., 2020. VR-Experience (ML-Agents) [WWW Document] [Online; accessed yyyy-mm-dd]. URL https://ddhaese.github.io/vr-experience/ © 2020 David D’Haese Bronvermelding "],
["beginselen-ml-agents.html", "Hoofdstuk 2 Beginselen ML-Agents 2.1 Vereiste voorkennis en voorinstallatie 2.2 Installatie 2.3 ML-Agents Toolkit 2.4 Machine learning in een notedop 2.5 Interactie tussen het NN en de Unity omgeving", " Hoofdstuk 2 Beginselen ML-Agents 2.1 Vereiste voorkennis en voorinstallatie Om onderstaande handleiding te kunnen volgen moet je eerst op Digitap de volgende modules hebben afgewerkt: Week 1 - Introduction Virtual Reality Week 2 - Introduction &amp; Unity Basics Week 3 - Unity - Materials &amp; lighting &amp; audio, camera &amp; deployment Week 4 - Physics , Player control &amp; UI Week 5 - Shooter Game De vereiste software voor dit onderdeel is Unity versie 2019.4.10. 2.2 Installatie Opgelet: De versie van de geïnstalleerde software speelt hier een belangrijke rol. ML-Agents is nog volop in ontwikkeling en de functionaliteiten kunnen tussen twee versies erg verschillen. Installeer ML Agents: Unity heeft wat problemen gehad met het versioneren van hun product, dus moet je goed opletten Technologies (2020)). Via de Package Manager van Unity installeer je ML Agents versie 1.0.5. Dit zou moeten overeenkomen met de GitHub Release 6 repository. Installeer Python: Wil je met een omgeving-manager werken zodat je in een afgezonderde omgeving kunt werken met zijn eigen specifieke set aan Python modules, dan kan je Anaconda of de vereenvoudigde versie Miniconda downloaden in een folder waarvan het pad geen spaties bevat. Je test deze installatie met het commando conda info. Je kan een ontwikkel omgeving (eng: development environment) maken met het commando conda create -n ml-agents en deze omgeving activeren met conda activate ml-agents en deactiveren met conda deactivate. Met de ml-agents omgeving actief kan je met de opdracht python --version controleren dat de versie van Python voldoende recent is (3.8.1 of hoger) en met pip install mlagents installeer je de mlagents Python module Indien de geen omgeving-manager wenst te gebruiken of de installatie hiervan werkt niet goed op jouw machine, installeer dan gewoonweg de nieuwste versies van Python en pip en voer de opdracht pip install mlagents uit aan de prompt. (Optioneel, maar sterk aangeraden) Installeer voorbeelden: Om toegang te hebben tot een aantal eenvoudige voorbeelden kopieer je best de repository met volgend git commando: git clone --branch release_6 https://github.com/Unity-Technologies/ml-agents.git Een gedetailleerde handleiding vind je hier. 2.3 ML-Agents Toolkit De Unity Machine Learning Agents Toolkit (of kortweg ML-Agents) stelt de spel- of simulatie-ontwikkelaar in staat om één of meerdere karakters (eng: non-player characters of NPCs) of agenten (eng: agents) intelligent te maken door deze een virtueel brein mee te geven. Je bepaalt zelf wat je de agenten aanleert en dus wat er in het brein terechtkomt. Als het brein voldoende complex is en als er voldoende leertijd voorzien wordt zijn er bijna geen limieten aan wat je een agent kunt aanleren. Hier is een voorbeeld: De toepassingen zijn legio: Single-Agent: Je traint één agent om iets uit zichzelf te leren op basis van observaties die de agent doet (zoals de overstekende kip uit de video hierboven). Simultaneous Single-Agent: Meerdere onafhankelijke karakters gekoppeld aan eenzelfde brein. Adversarial Self-Play: Meerdere karakters met hetzelfde brein maar tegenovergesteld beloningssysteem zodat de agent van zichzelf leert. Deze strategie is toegepast bij AlphaGo Cooperative Multi-Agent: Meerdere agenten die leren samenwerken doordat ze een beloningssysteem delen. Competitive Multi-Agent: Meerdere teams van interagerende agenten. Het soccer voorbeeld illustreert deze strategie. Ecosystem: Complex systeem met meerdere agenten elke met onafhankelijke beloningssystemen. De onderstaande figuur biedt een overzicht van de elementen in de ML-Agents toolkit. Figuur 2.1: Overzicht van de elementen van de ML-Agents toolkit. De Academy zorgt ervoor dat de agenten gesynchroniseerd zijn en beheert de nodige omgeving-brede instellingen. Gebaseerd op bron. 2.4 Machine learning in een notedop Machine learning (ML) is het vermogen van een algoritme om te leren uit data. Centraal in het proces-diagram van ML bevindt zich het leeralgoritme dat data omzet naar een model. Dit model kan men beschouwen als een functie dat op basis van gelijkaardige data nieuwe voorspellingen kan doen. Een model bestaat uit logica en parameters. Figuur 2.2: Vereenvoudigd proces-diagram voor ML. Cirkels stellen acties voor, rechthoeken stellen objecten of statussen voor. Het leeralgoritme kan dus beschouwd worden als een function-factory. Het ML proces kan geconfigureerd worden door middel van hyperparameters. Stelling 2.1 Machine learning (ML) is het vermogen van een algoritme om te leren uit data. Er zijn verscheidene vormen, maar het komt er meestal op neer dat op basis van data een model wordt gemaakt. De vorm van ML waar we hier mee in contact komen is conditionering (eng: reinforcement learning) Zulk een conditionering-model kan men beschouwen als een functie die op basis van een bepaalde situatie de meest aangewezen actie probeert te voorspellen op basis van een beloning-systeem. Bijvoorbeeld, het model moet aanleren dat bij de situatie: …er is een wagen rechts van me die met hoge snelheid nadert… een gepaste actie zou kunnen zijn: Zet snel twee stappen naar voren Bij de situatie: …er staat een boom vlak voor mijn neus… zou dan weer de volgende actie passen: Zet een stap opzij. Bij een conditionering bestaat de data dus uit de situaties (algemeen de set van de toestanden waarin de verscheidene agenten zich bevinden) en de beloningen die worden uitgereikt op basis van eerdere genomen acties. Figuur 2.3: Vergelijk met Figuur 2.2, de data bestaat uit een situatie (een set van statussen) en een beloning op de acties die door het model in spe eerder werden voorgesteld. De standaard levenscyclus van een unity-omgeving (eng: environment) wordt tijdens het trainen van de agenten uitgebreid het doorlopen van de zogenaamde epochs (ook wel steps) van het NN. De twee systemen, TensorFlow en Unity, hebben wel een zekere onafhankelijkheid en communiceren aan de hand van buffers. Zo bestaat er een experience buffer die de observaties vanuit Unity verzameld en van waaruit het NN ten gepaste tijde de invoerlaag mee voed. 2.5 Interactie tussen het NN en de Unity omgeving Tijdens de trainingsfase zal het NN acties voorstellen. In het begin zullen dit willekeurige acties zijn en als alles goed loopt, worden de acties gaandeweg ‘intelligenter’. De acties worden vanuit het NN naar Unity verstuurd waar deze (in de OnActionReceived methode, zie later) worden ontvangen, omgezet naar werkelijke bewegingen (of andere status wijzigingen) van de agent en vertaald naar een beloning. Op geregelde tijdstippen wordt de geaccumuleerde beloning tenslotte vanuit Unity naar het NN verstuurd alwaar ze gebruikt zullen worden om de parameters (‘connecties’) van het NN op geschikte wijze aan te passen. Hoe dit laatste precies gebeurt, valt buiten het bereik van deze korte introductie. Figuur 2.4: Een detail van het conditioneringsproces. Het model wordt in dit geval door Python afgehandeld en er is een Python Low-Level API om te communiceren tussen Python en de Unity Academy. Indien een leeralgoritme van het conditionering-type gebruik maakt van neurale netwerken, spreekt men van deep reinforcement learning. Het model ziet er dan als volgt uit: Figuur 2.5: Neuraal netwerk (NN) toegespitst op conditionering. De parameters zijn de gewichten van de connectoren. De verschillende gewichten worden hier voorgesteld met verschillende lijndiktes voor de connectoren. De positie van de agent dient hier als voorbeeld, men kiest vrij welke data als invoer dienen voor het netwerk. Informatie vloeit van links naar rechts. Links komen de observaties en de statussen binnen. Samen worden deze hier de situatie genoemd (ook wel ervaring, eng: experience) en hiermee wordt de invoer-laag (eng: input layer) van het netwerk opgevuld. Via allerhande matrix vermenigvuldigingen bekomt men de voorspelde acties uit, hier gecodeerd door middel van drie natuurlijke getallen. De acties worden naar de Unity-omgeving gestuurd en daar gaan deze acties dus omgezet worden naar status-wijzigingen van het spelobject van de agent. Bijvoorbeeld, de agent zet een stap naar voren. Indien, als gevolg van deze stap naar voren, de agent in botsing treedt (eng: collision) met een object en het daarvoor ook beloond wordt, vloeit deze beloning terug naar het NN waar de parameters aangepast worden waardoor in de toekomst het NN aangemoedigd wordt om in dezelfde omstandigheden opnieuw een stap naar voren te zetten. Bronvermelding "],
["obelix-de-menhirhouwer.html", "Hoofdstuk 3 Obelix de menhirhouwer 3.1 Inleiding tot de usecase 3.2 Het spelverloop 3.3 Observaties, acties en beloning 3.4 Het speelveld 3.5 Spelomgeving", " Hoofdstuk 3 Obelix de menhirhouwer 3.1 Inleiding tot de usecase Vele mensen kennen de verhalen van Asterix en Obelix. Hierin speelt Obelix de rol van een menhirhouwer met bovennatuurlijke krachten. Spijtig genoeg is hij nogal naïef en doet hij wel eens domme dingen. Laten we daar verandering in brengen door Obelix een brein te geven! Figuur 3.1: Kunnen we Obelix de menhirhouwer een tikkeltje slimmer maken? bron afbeelding van Obelix. 3.2 Het spelverloop Het spelverloop wordt hieronder schematisch voorgesteld. Obelix is het hoofdpersonage en moet op zoek naar vrije menhirs. Aangekomen bij een menhir moet hij deze op zijn rug nemen en op zoek gaan naar een bestemming. Figuur 3.2: Het spelverloop hier uitgebeeld voor een enkele vrije, stilstaande menhir. 3.3 Observaties, acties en beloning Er zijn drie zaken waarover je bij een conditionering altijd eerst rustig moet nadenken alvorens als een gek te beginnen ontwikkelen of ontwerpen: observaties, acties en beloning. Stelling 3.1 (Observaties) Observaties die de agent maakt op basis van de omgeving vormen de invoer voor het leeralgoritme. In het geval van Obelix zijn de observaties diegene die hij maakt door rond te ‘kijken’. We gaan zien dat ML-Agents ons hierbij helpt door componenten aan te bieden voor visuele perceptie. Stelling 3.2 (Acties) Acties bepalen de bewegingsvrijheid van een agent of, algemener gesteld, de impact die een agent heeft op zijn omgeving. Zolang de agent niet veranderd in geen enkel van zijn eigenschappen (positie, snelheid, kleur, tags,…) dan kan het leeralgoritme daar niets mee, want wat het ook voorstelt als te ondernemen actie, er gebeurt toch niets. Voor Obelix is het al duidelijk geworden dat hij moet kunnen rondkijken om ervoor te zorgen dat er een menhir of bestemming in het gezichtsveld treedt. Verder moet hij kunnen ‘rondlopen’ zodat hij een menhir of bestemming kan benaderen. Als we er van uitgaan dat het opnemen van de menhir en daarna het lossen van de menhir op een bestemming volautomatisch gebeurt (i.e. hij moet er gewoon maar mee botsen) dan zijn er geen verdere acties die Obelix moeten kunnen ondernemen. De details van het roteren en het rondlopen bespreken we later. Stelling 3.3 (Beloning) Het beloningsmechanisme (eng: rewards, incentives) vertelt het leeralgoritme of de voorgestelde actie de agent dichter bij het einddoel van de leeroefening brengt of niet. Figuur 3.3 geeft een overzicht van het beloningssysteem voor de Obelix simulatie. Het einddoel is het plaatsen van de menhir op een bestemming en dus daar krijgt het leeralgoritme de grootste score voor. Voor het naderen van een menhir of een bestemming krijgt de agent enkel een (matige) beloning indien het zich in de juiste staat bevindt. Bijvoorbeeld, als Obelix al een steen op zijn rug draagt, krijgt hij geen extra punten om een andere menhir te benaderen, want twee menhirs op de rug is misschien toch wat veel van het goede (in deze versie, natuurlijk kan je dit aanpassen). Figuur 3.3: Het beloningssysteem voor de Obelix-simulatie. 3.4 Het speelveld Laten we beginnen met alle componenten te bouwen om de simulatie te bouwen. Merk op dat het verstandig is om eerst de ML logica in te bouwen alvorens je de spelobjecten tot in het detail uitwerkt. Dat is ook wat we hier gaan doen. Figuur 3.4: Het speelveld met 5 spelobjecten. 3.5 Spelomgeving 3.5.1 Speelveld object Het speelveld krijgt de naam Henge en is een eenvoudig vlak met schaal X = Y = Z = 2 en posities en rotatie op X = Y = Z = 0 en met een Mesh Collider. Dit en alle andere objecten zitten vervat in een container object Environment zodat het geheel later eenvoudig gedupliceerd kan worden. Figuur 3.5: De volledige hiërarchie binnen de spelobjecten met hun namen zoals ze in deze handleiding gebruikt zullen worden. Merk op dat de Menhir en Destination objecten als prefab zullen worden gedefinieerd zodat men hier meerdere instanties van kan genereren (eng: to spawn) tijdens de initialisatie van de simulatie. 3.5.2 Obelix spelobject Figuur 3.6: Obelix spelobject. Zie tekst voor de eigenschappen van dit object. Dit is het enige composiet spelobject met het cilindrisch lichaam Obelix, een rechteroog (RightEye), een linkeroog (LeftEye) en een neus (Nose). Het lichaam zelf is een capsule met schaal 1 in alle richtingen. Het bevat een passende Capsule collider en een Rigid body component met de rotatie-vrijheid beperkt tot enkel de y-as (dus freeze rotation X en Z aangevinkt). De ogen en de neus worden als dochter-componenten van het lichaam aangemaakt en bestaan uit bollen, respectievelijk een capsule, telkens met hun gepaste en aansluitende collider. Zorg dat het composiet object voldoende boven de grond is (y = 1.5) zodat hij netjes op het speelveld valt. De acties die Obelix gaat doen zijn gebaseerd op observaties. Zorg er dan ook voor dat beide ogen van Obelix kunnen ‘zien’ door aan beide ogen een Ray Perception Sensor 3D toe te voegen met de volgende eigenschappen (hier voor het links oog weergegeven). Eigenschap Waarde Uitleg Sensor Name LeftEye Unieke naam is belangrijk Detecable Tags 2: Menhir en Destination De objecten die Obelix kan zien Rays Per Direction 6 Aantal stralen aan weerszijden van middellijn. Max Ray Degrees 30 Voor een zicht van 60° per oog. Sphere Cast Radius 3.5 De dikte van elke straal Ray Length 260 Lengte van de stralen in dm Stacked Raycasts 1 Geheugen voor de sensor Start/End Vertical Offset 0/0 Naar boven of naar onder kijken Obelix krijgt dus een zicht van 120°. Hij zal zich dus moeten draaien om een volledig beeld te krijgen van zijn omgeving. De lengte van de stralen is zodanig gekozen dat hij van de ene hoek van het speelveld nog net een object in de tegenoverstaande hoek kan zien. Doe hetzelfde voor het rechteroog (je kan de component eenvoudig kopiëren). Roteer nu de ogen +/-30° volgens de Y-as zoals getoond op Figuur 3.7 zodat er een zo breed mogelijk gezichtsveld benut wordt (dus geen overlap). Figuur 3.7: De Ray Perception Sensor 3D component in actie. Er zijn ook andere componenten die van de betrokken ISensor interface overerven, namelijk CameraSensorComponent waarmee camerabeelden als pixel-matrices aan het NN worden doorgegeven of RenderTextureSensorComponent die de inhoude van zogenaamde Render Textures als invoer gebruikt voor het NN. Voeg verder twee componenten toe aan Obelix: Behavior Parameters (het gedrag van de agent) en Decision Requester. (een automatische trigger om de agent te dwingen iets te doen). We zullen deze componenten waar nodig gaandeweg configureren. 3.5.3 De menhir en de bestemming De menhir en de bestemming prefabs worden eenvoudig geconstrueerd als kubussen met schaal {XYZ: 0.5, 2, 0.5} met een passende Box Collider. De verschillen tussen de Menhir prefab en de Destination prefab staan hieronder opgesomd: Figuur 3.8: De menhir bestaat uit een eenvoudige balk. Eigenschap Menhir Destination Rigidbody component Use Gravity aan en Freeze Rotation voor de assen X en Z Geen Rigidbody component. Startpositie (Y) Boven het veld, valt bij opstart tot op speelveld Rustend op het speelveld Tag Menhir Destination Je kan ze natuurlijk best ook met een verschillend materiaal bekleden zodat ze gemakkelijk te onderscheiden zijn. 3.5.4 Scorebord Dit object met naam ScoreBoard is een instantie van de TextMeshPro klasse (3D-Object) dat de gecumuleerde beloning (eng: cumulative reward) zal weergeven. Hier wijzig je de eigenschappen naar keuze. Het is weliswaar handig als je de score kan lezen vanuit het perspectief van de main camera. "],
["gedragingen-van-de-agent-en-de-andere-spelobjecten.html", "Hoofdstuk 4 Gedragingen van de agent en de andere spelobjecten 4.1 Environment.cs 4.2 Destination.cs 4.3 Obelix.cs 4.4 Menhir.cs", " Hoofdstuk 4 Gedragingen van de agent en de andere spelobjecten 4.1 Environment.cs Wil je een agent een complexe handeling aanleren, i.e. een handeling dat je niet eenvoudig kan programmeren, dan zal je typisch de agent willen blootstellen aan een onvoorziene situatie. Wie onvoorzien zegt, zegt ook willekeurig en in Environment.cs bestaat een belangrijk deel van de code er dan ook in om de menhirs willekeurig op het speelveld te plaatsen. Kwestie om Obelix het niet te gemakkelijk te maken. 4.1.1 Overzicht methoden op Environment Maak een script met deze naam en koppel het aan het gelijknamig spelobject. De Environment klasse zal De volgende methoden bevatten: randomPosition: het genereren van een willekeurige positie in het XZ vlak. clearEnvironment: het opkuisen van eventuele spelobjecten van een vorige episode spawnStoneHenge: het genereren van bestemmingen, netjes in een cirkel zoals bij Stonehenge (ongeveer toch) spawnMenhirs: het genereren van de menhirs 4.1.2 Object-variabelen op Environment We beginnen bij een aantal publieke object-variabelen: public Destination destinationPrefab; public Menhir menhirPrefab; public Material matMenhirInPlace; public int menhirCount = 8; public float circleRadiusDestinations = 9; De prefabs moeten hier door de ontwikkelaar manueel gekoppeld via de Unity GUI zodat je met een eenvoudige sleepbeweging een andere menhir of bestemming kunt koppelen. Het materiaal matMenhirInPlace zal dienen om aan te geven wanneer een bestemming bezet wordt door een menhir en is bedoeld enkel voor de ontwikkelaar (dus niet cruciaal voor de simulatie zelf). menhirCount is een simulatie-parameter die bepaalt hoeveel {menhir, bestemming}-koppels er gecreëerd dienen te worden en circleRadiusDestinations bepaalt de straal van de Stonehenge. Door beide parameters publiek te maken, kan de ontwikkelaar ze eenvoudig wijzigen via de GUI van Unity. De volgende variabelen zijn privaat en zorgen ervoor dat de simulatie-omgeving een referentie heeft naar de agent (obelix), het scorebord (scoreBoard) en de container-objecten voor de menhirs (menhirs) en bestemmingen (destinations). private Obelix obelix; private TextMeshPro scoreBoard; private GameObject menhirs; private GameObject destinations; 4.1.3 Initializatie van Environment instantie Tijdens de initialisatie zullen de bovenstaande referenties worden ingevuld: public void OnEnable() { destinations = transform.Find(&quot;Destinations&quot;).gameObject; menhirs = transform.Find(&quot;Menhirs&quot;).gameObject; scoreBoard = transform.GetComponentInChildren&lt;TextMeshPro&gt;(); obelix = transform.GetComponentInChildren&lt;Obelix&gt;(); } Merk op dat de Find en GetComponentInChildren methoden op de transform van Environment ervoor moeten zorgen dat uitsluitend dochter-objecten worden teruggegeven. Dit is belangrijk omdat we later de hele simulatie-omgeving gaan dupliceren binnen dezelfde scene. Andere methoden zoals GameObject.FindGameObjectWithTag zouden dus mogelijk de verkeerde referenties doorgeven, i.e. referenties naar objecten van een andere simulatie-omgeving, zonder hiervoor een foutmelding te geven. 4.1.4 Opkuisen speelveld Bij het begin van elke episode zal het veld opnieuw in zijn beginsituatie moeten worden alvorens nieuwe menhirs en bestemmingen te genereren: public void clearEnvironment() { foreach (Transform menhir in menhirs.transform) { GameObject.Destroy(menhir.gameObject); } foreach (Transform destination in destinations.transform) { GameObject.Destroy(destination.gameObject); } } 4.1.5 Score Het scorebord moet continu de beloning (= score) weergeven en dit gebeurt eenvoudigweg door gebruik te maken van de getter van de interne m_CumulativeReward variabele op de Agent klasse: private void FixedUpdate() { scoreBoard.text = obelix.GetCumulativeReward().ToString(&quot;f2&quot;); } 4.1.6 Willekeurige positie Het is belangrijk om het trainingsfase zo representatief mogelijk te maken voor de latere situaties waarmee het NN geconfronteerd zal worden. Hier is het belangrijk dat Obelix de menhirs op hun bestemming weet te brengen, onafhankelijk van hun positie en van de richting waarin ze zich voortbewegen. De code voor het bepalen van een willekeurige positie ziet er als volgt uit: public Vector3 randomPosition(float up) { float x = Random.Range(-9.75f, 9.75f); float z = Random.Range(-9.75f, 9.75f); return new Vector3(x, up, z); } Aangezien de menhirs en bestemmingen een zijde van 0.5 hebben, mogen ze tot op de helft daarvan de rand van het speelveld naderen. 4.1.7 Genereren van een Stonehenge Zoals eerder vermeld, is het de bedoeling dat Obelix de menhirs in een cirkel plaatst. De bestemmingen (Destination) worden dan ook gelijkmatig verdeeld over de cirkel met straal circleRadiusDestinations. Figuur 4.1: Opfrissing basis driehoeksmeetkunde. Er wordt bovendien ook voor gezorgd dat de bestemmingen netjes gegroepeerd blijven binnen de Destinations container. Merk op dat de Mathf functies met radialen werken terwijl Quaternion.Euler graden ([0, 360]) aanvaardt. public void spawnStoneHenge() { for (int i = 0; i &lt; menhirCount; i++) { GameObject newDestination = Instantiate(destinationPrefab.gameObject); float angle = (float)i / (float)menhirCount * 2f * Mathf.PI; float x = circleRadiusDestinations * Mathf.Cos(angle) + transform.position.x; float z = circleRadiusDestinations * Mathf.Sin(angle) + transform.position.z; newDestination.transform.localPosition = new Vector3(x, 1f + transform.position.y, z); newDestination.transform.localRotation = Quaternion.Euler(0f, angle / Mathf.PI * 180, 0f); newDestination.transform.SetParent(destinations.transform); } } 4.1.8 Dynamisch genereren van menhirs Tenslotte rest ons nog de menhirs willekeurig over het veld te verdelen. De bewegingen van de menhirs worden later door de Menhir-klasse zelf verzorgd. public void spawnMenhirs() { for (int i = 0; i &lt; menhirCount; i++) { GameObject newMenhir = Instantiate(menhirPrefab.gameObject); newMenhir.transform.SetParent(menhirs.transform); newMenhir.transform.localPosition = randomPosition(1f); newMenhir.transform.localRotation = Quaternion.Euler(0f, Random.Range(0f, 360f), 0f); } } 4.2 Destination.cs Allicht het meest eenvoudige script, kwestie om de klasse te koppelen aan het juiste spelobject: using UnityEngine; public class Destination : MonoBehaviour { } 4.3 Obelix.cs 4.3.1 Overzicht methoden De Obelix klasse is waar de echte actie plaatsvindt, natuurlijk. Je maakt van Obelix een agent simpelweg door deze te laten overerven van de Agent klasse: public class Obelix : Agent { } Deze klasse bevat 6 methoden. Initialize: Eenmalige initialisatie van de agent OnEpisodeBegin: De initialisatie bij de aanvang van een episode CollectObservations: Het verzamelen van observaties uit de omgeving Heuristic: Indien er geen NN gekoppeld is, dan zorgt deze methode voor een alternatieve manier voor het bepalen van de acties die de agent moet nemen, bijvoorbeeld via toetsaanslagen OnActionReceived: De wijzigingen die het spelobject van de agent moet ondergaan wanneer de speler (via Heuristic) óf het NN voorstelt om een bepaalde actie uit te voeren. Bijvoorbeeld “vooruit” → transform.Translate(...) OnCollisionEnter: De acties die uitgevoerd moeten en beloningen die uitgereikt dienen te worden wanneer de agent ergens tegen aanbotst. Behalve OnCollisionEnter erven al deze methoden over van de Agent klasse. Merk op dat het aanroepen van de RequestDecision methode in de nieuwere versies van Unity wordt overgenomen door de Component Decision Requester. Denk er dus aan om deze component aan Obelix toe te voegen via de Unity GUI. Doordat in sommige gevallen Unity een Agent.cs script automatisch toevoegt, eindig je met een agent waaraan twee script-componenten gekoppeld zijn. Dit moet je vermijden, zorg dat enkel jouw script, nl. Obelix.cs, aan de agent gekoppeld is. 4.3.2 Objectvariabelen public float speed = 10; public float rotationSpeed = 350; private Rigidbody body; private Environment environment; private Material matMenhirInPlace; private int menhirCount; private float carriesMenhir = 0; private int menhirsDone = 0; De variabelen speed en rotationSpeed staan in voor respectievelijk de translatie en rotatie-snelheid van onze Obelix en kunnen via de GUI worden aangepast. De variabelen body, environment, matMenhirInPlace zorgen voor de nodige referenties, kwestie om niet steeds de objecten in de object-hiërarchie te moeten opzoeken. De laatste drie variabelen staan in om het spelverloop bij te houden. carriesMenhir geeft aan of Obelix op dit moment een menhir aan het dragen is en menhirsDone houdt bij hoeveel menhirs er reeds op hun bestemming zijn gezet. 4.3.3 Initialize De eenmalige initialisatie van onze Obelix agent ziet er als volgt uit: public override void Initialize() { base.Initialize(); body = GetComponent&lt;Rigidbody&gt;(); environment = GetComponentInParent&lt;Environment&gt;(); matMenhirInPlace = environment.matMenhirInPlace; } 4.3.4 OnEpisodeBegin Bij de aanvang van elke episode, moet Obelix opnieuw gepositioneerd worden (hij vertrekt van stilstand), moet het speelveld geruimd worden en moeten er nieuwe menhirs en overeenkomstige bestemmingen gegenereerd worden. public override void OnEpisodeBegin() { transform.localPosition = new Vector3(0f, 1.5f, 0f); transform.localRotation = Quaternion.Euler(0f, 0f, 0f); body.angularVelocity = Vector3.zero; body.velocity = Vector3.zero; environment.clearEnvironment(); environment.spawnMenhirs(); environment.spawnStoneHenge(); menhirsDone = 0; menhirCount = environment.menhirCount; } 4.3.5 CollectObservations In deze cruciale methode gebeurt het observeren. De eerste bron van observaties is het zicht van Obelix. Dit wordt verzorgd door de twee Ray Perception Sensor 3D componenten, één voor elk oog van onze agent. Door het toevoegen van deze componenten wordt de Academy automatisch op de hoogte gebracht van de extra observaties en hiervoor moet de ontwikkelaar dus geen verdere actie ondernemen. Deze observatie moet Obelix wel nog meegeven: het feit of hij een menhir op zijn rug heeft of niet, omdat dit belangrijk is voor het spelverloop, natuurlijk. public override void CollectObservations(VectorSensor sensor) { sensor.AddObservation(carriesMenhir); if (transform.localPosition.y &lt; 0) { AddReward(-1f); EndEpisode(); } } Onderaan werd er een clausule toegevoegd om ervoor te zorgen dat Obelix de afgesproken afstraffing krijgt (i.e. negatieve beloning met 1 punt) indien hij zich buiten het speelveld begeeft en door de zwaartekracht naar beneden valt. 4.3.6 Heuristic Deze methode helpt om de correcte werking en beloning van Obelix te testen terwijl hij in de simulatie zit en allerhande acties uitvoert. De Heuristic-methode is geen verplichting maar bespaart heel wat debug tijd achteraf, dus zeker een aanrader: public override void Heuristic(float[] actionsOut) { actionsOut[0] = 0f; actionsOut[1] = 0f; if (Input.GetKey(KeyCode.UpArrow)) // Moving fwd { actionsOut[0] = 2f; } else if (Input.GetKey(KeyCode.DownArrow)) // Turning left { actionsOut[0] = 1f; } else if (Input.GetKey(KeyCode.LeftArrow)) // Turning left { actionsOut[1] = 1f; } else if (Input.GetKey(KeyCode.RightArrow)) // Turning right { actionsOut[1] = 2f; } } In dit geval kan je Obelix manueel besturen met de pijltjes toetsen. Het gedrag van de agent (via de pijltjes toetsen of via NN) kan je via de Behavior Type parameter van de Behavior Parameters component van de agent in de GUI van Unity aanpassen: Gebruik Heurstic om te testen, Inference om Obelix te besturen d.m.v. een eerder getraind brein of Default waarbij de Academy zelf gaat onderzoeken of er een NN aan het wachten is voor input en zo ja de trainings-modus in gaat. 4.3.7 OnActionReceived Deze methode staat in voor het vertalen van een voorgestelde actie naar bewegingen of andere wijzigingen van het spelobject dat met de agent gekoppeld is. Acties worden als een getallenreeks gecodeerd en als dusdanig doorgegeven vanuit het NN of via de Heuristic-methode. Voor Obelix is ervoor gekozen om discrete acties te ontvangen (t.o.v. continue waarden). public override void OnActionReceived(float[] vectorAction) { if (vectorAction[0] == 0 &amp; vectorAction[1] == 0) { AddReward(-0.001f); return; } if (vectorAction[0] != 0) { Vector3 translation = transform.forward * speed * (vectorAction[0] * 2 - 3) * Time.deltaTime; transform.Translate(translation, Space.World); } if (vectorAction[1] != 0) { float rotation = rotationSpeed * (vectorAction[1] * 2 - 3) * Time.deltaTime; transform.Rotate(0, rotation, 0); } } De structuur van de actie-vector is erg eenvoudig. Deze bestaat uit twee natuurlijke getallen. Het eerst bepaalt of er een stap naar voren of naar achteren wordt genomen, het tweede of er naar links of naar rechts wordt gedraaid: vectorAction[0]: 0: stilstand 1: stap naar achteren 2: stap naar voren vectorAction[1]: 0: niet draaien 1: draai naar rechts 2: draai naar links Omdat 0 de standaardwaarde is, is het best om deze te behouden voor ‘het niets doen’ van de agent. Het definiëren van de structuur van vectorAction gebeurt in de Behavior Parameters component: In de methode OnActionReceived wordt meestal (een deel van) de beloning afgehandeld. Hier zien we dat als Obelix niets doet, hij hiervoor wordt afgestraft met een negatieve beloning van 0.001 punt (zie Figuur 3.3 voor meer info). Andere beloningen worden gegeven tijdens het botsen met menhirs of bestemmingen (zie volgende §). Nog één opmerking: In dit geval zet de agent zelf een stap, maar je kan dit natuurlijk ook vervangen door een kracht (rigidBody.AddForce()) die inwerkt op je agent. De actie hoeft trouwens helemaal niet per sé gekoppeld aan een beweging. Het kan gaan om een materiaal wissel (bijv. leren om invisibility cloak te gebruiken wanneer vijand kijkt), status wijziging (bijv. leren om strategie van de agent zoals roekeloosheid aan de situatie aan de passen), identiteitswissel (door tags dynamisch te wijzigen), enz… 4.3.8 OnCollisionEnter Botst Obelix op een menhir,en draagt hij nog geen menhir op zijn rug, dan ontvangt hij 0.1 punt, wordt zijn status-veld carriesMenhir aangepast en wordt de menhir in kwestie verwijderd van het speelveld. Botst Obelix op een bestemming, dan krijgt hij 1 punt en wordt de bestemming aangepast zodat zij niet langer als bestemming kan dienen (aanpassen van de tag eigenschap). Er wordt hier een extra afstraffing toegevoegd van -0.1 punt, namelijk wanneer Obelix een menhir raakt wanneer hij reeds een ander menhir op de rug draagt. void OnCollisionEnter(Collision collision) { if (collision.transform.CompareTag(&quot;Destination&quot;) &amp; carriesMenhir == 1f) { AddReward(1f); carriesMenhir = 0; collision.gameObject.tag = &quot;MenhirInPlace&quot;; collision.gameObject.GetComponent&lt;Renderer&gt;().material = matMenhirInPlace; menhirsDone++; if (menhirsDone == menhirCount) { EndEpisode(); } } else if (collision.transform.CompareTag(&quot;Menhir&quot;) &amp; carriesMenhir == 0) { AddReward(0.1f); Destroy(collision.gameObject); carriesMenhir = 1f; } else if (collision.transform.CompareTag(&quot;Menhir&quot;) &amp; carriesMenhir == 1f) { AddReward(-0.1f); } } Merk op dat de CompareTag()-methode wordt gebruikt om de identiteit van het object te achterhalen waarmee Obelix in botsing treedt, dezelfde manier waarop de Ray Perception Sensor 3D componenten hun observaties doorgeven. Merk ook op dat de episode beëindigd wordt wanneer alle menhirs op hun plaats werden gebracht. 4.4 Menhir.cs Rest ons enkel nog om de menhir in beweging te zetten. Tijdens elke doorgang in de FixedUpdate methode zet de menhir een stap. De richting waarin de menhir zich voortbeweegt, wordt bepaald door targetPosition. Dit is een willekeurig gekozen punt binnen het speelveld waarnaar de menhir zich begeeft. Hoewel dat hier niet zo belangrijk is, richt de menhir zich ook werkelijk naar zijn doel. Er wordt berekend hoelang het zou duren voordat de menhir zijn doel zou bereiken en bij het verstrijken van die tijdspanne wordt er een nieuwe targetPosition uitgekozen en begint de cyclus opnieuw. Verder wordt de snelheid gerandomiseerd in het bereik [\\(\\frac12v\\), \\(\\frac32v\\)], waarbij \\(v\\) de opgegeven gemiddelde snelheid speed is. using UnityEngine; public class Menhir : MonoBehaviour { public float speed = 1; private float randomizedSpeed = 0f; private float nextActionTime = -1f; private Vector3 targetPosition; private Environment environment; private void FixedUpdate () { if (speed &gt; 0f) { move (); } } private void move () { if (environment == null) { environment = GetComponentInParent&lt;Environment&gt; (); } if (Time.fixedTime &gt;= nextActionTime) { randomizedSpeed = speed * Random.Range (.5f, 1.5f); targetPosition = environment.randomPosition (1f); transform.rotation = Quaternion.LookRotation (targetPosition - transform.localPosition, Vector3.up); float timeToGetThere = Vector3.Distance (transform.localPosition, targetPosition) / randomizedSpeed; nextActionTime = Time.fixedTime + timeToGetThere; } else { Vector3 moveVector = randomizedSpeed * transform.forward * Time.fixedDeltaTime; if (moveVector.magnitude &lt;= Vector3.Distance (transform.localPosition, targetPosition)) { transform.localPosition += moveVector; } else { transform.localPosition = targetPosition; nextActionTime = Time.fixedTime; } } } private void OnCollisionEnter (Collision collision) { nextActionTime = Time.fixedTime; } } Merk op dat er een controle is ingebouwd indien de menhir voorbij zijn doel schiet en dat er bij het botsen met een ander object ook een nieuw doel wordt uitgekozen om de kans te verminderen dat een menhir als gevolg van een botsing buiten het speelveld zou terecht komen. "],
["testen-van-de-spellogica.html", "Hoofdstuk 5 Testen van de spellogica", " Hoofdstuk 5 Testen van de spellogica Je kan nu de spellogica beginnen testen door zelf Obelix voort te bewegen d.m.v. de pijltjestoetsen. Controleer dat de instelling Behavior Type op Heuristic of Default staat. Je kan nu met de pijltjes toetsen het spel manueel bedienen (dus menselijke brein i.p.v. artificieel NN). Probeer manueel een menhir te vangen en naar hun bestemming te brengen, pas desnoods de snelheid van de menhirs aan. Controleer het scorebord. Schakel tijdelijk de afstraffing voor het niets doen uit door de statement AddReward(-0.001f); in commentaar te plaatsen zodat je de tijd krijgt om te controleren dat de juiste beloning bij de juiste acties worden gegeven (vergeet daarna niet terug te zetten!). Controleer ook eens een nieuwe episode gestart wordt indien Obelix van het speelveld valt of indien alle menhir op hun plaats zijn (je kan het aantal aanpassen). Stelling 5.1 (Eerst manueel testen) Begin pas met de trainingsfase indien je 100% bent dat de spellogica helemaal goed zit. "],
["trainen-van-het-nn.html", "Hoofdstuk 6 Trainen van het NN 6.1 Configuratie van het NN 6.2 Start de training 6.3 Paralleliseren", " Hoofdstuk 6 Trainen van het NN 6.1 Configuratie van het NN Configuratie van het NN en de Python-Unity interface gebeurt hier d.m.v. een yaml-bestand (cheatsheet hier). Maak daarom een nieuw bestand aan met de naam Obelix-01.yml. We beginnen met aan te geven dat de configuratie geldt voor de Obelix agent: behaviors: Obelix: Opgelet: De naam Obelix moet overeenkomen met de Behavior Name. Je kan in het configuratiebestand meerdere gedragingen toevoegen, waarbij een gedrag overeenkomt met type agent en met een NN. 6.1.1 Trainer-type De eerste configuratie-parameter is trainer_type: ppo Hiermee bepaal je het type conditioneringsalgoritme en meteen ook een groot aantal andere confirguratieparameters. De mogelijkheden zijn: ppo: Proximal Policy Optimization Dit algoritme zal trachten niet te ver af te wijken van de vorige set van voorgestelde acties (eng: policy) door een vorm van regularisatie toe te passen sac: Soft-Actor Critic Is een zogenaamde off-policy algoritme dat uit zijn geheugen ervaringen uit het verleden kan ophalen. Het heeft minder cycli nodig om te leren en is beter geschikt voor meer complexe omgevingen. 6.1.2 Hyperparameters Hieronder de hyperparameters op het leeralgoritme: max_steps: 5.0e5 time_horizon: 64 summary_freq: 10000 keep_checkpoints: 1 checkpoint_interval: 50000 hyperparameters: batch_size: 64 buffer_size: 9600 learning_rate: 3.0e-4 learning_rate_schedule: constant beta: 5.0e-3 epsilon: 0.2 lambd: 0.95 num_epoch: 3 max_steps: totaal maximum aantal stappen de simulatie mag doorlopen gedurende de ganse trainingsfase time_horizon: aantal ervaringen samen te nemen alvorens deze naar de ervaringsbuffer te sturen. Een set van observaties samen met de status waarin de agent zich bevindt, dus een input-rij voor het NN, noemt men de ervaring (eng: experience). summary_freq: aantal ervaringen die worden samengenomen alvorens samenvattende statistieken ervan te berekenen (≈ resolutie in Tensorboard) keep_checkpoints: Een checkpoint is een soort backup van het NN model checkpoint_interval: de frequentie waarmee de backups genomen moeten worden batch_size bepaalt de grootte van de minibatch. batch_size weerspiegelt het aantal ervaringen die worden samengenomen per optimalisatie-stap (gradiënt-afdaling, eng: gradient descent) buffer_size geeft het aantal ervaringen weer die worden samengenomen alvorens het leeralgoritme, in batches batch_size groot, de ervaringen verwerkt. Moet dus een meervoud van batch_size zijn learning_rate is de leersnelheid en bepaalt tijdens de optimalisatie de grootte van de stap die het leeralgoritme in de richting van de gradient neemt learning_rate_schedule: is de mate waarin de leersnelheid zich aanpast tijdens het leerproces. constant betekent dat de leersnelheid constant blijft en linear betekent dat de leersnelheid gelijkmatig afneemt naarmate het maximum aantal stappen (max_steps) nadert beta geeft weer hoe sterk het voorstel van een nieuw beleid wordt afgeremd en is een cruciaal onderdeel van de regularisatie van het beleid. Door middel van de entropie (Tensorboard) te volgen, kan men deze waarde bijsturen indien nodig. Daalt de entropie te snel t.o.v. een stijgende gemiddelde beloning → verhoog beta, daalt het te traag → verlaag beta epsilon is gelijkaardig aan beta maar bepaalt het maximum waarmee een nieuw beleid mag verschillen van het vorig voorgesteld beleid lambd is nog een regularisatie-parameter die bepaalt hoe sterk de beloningen doorwegen tijdens het aanpassen van de gewichten van de connectoren van het NN num_epoch bepaalt het aantal NN epochs per Unity frame 6.1.3 Architectuur van het NN Hieronder vind je de configuratie van de architectuur van het NN dat gekoppeld wordt met de Obelix agent: network_settings: num_layers: 2 hidden_units: 128 normalize: false num_layers: Het aantal verborgen lagen van het NN, dus exclusief de invoer en uitvoer laag hidden_units: Het aantal neuronen of nodes per verborgen laag in het NN. Samen met num_layers bepaald dit de grootte van het artificieel brein. normalize: Moet eigenlijk standardize heten en kan belangrijk zijn wanneer men acties d.m.v. continue variabelen codeert. Als je aan je eigen agent gaat werken, moet je ook nadenken of er geheugen nodig is. Zo ja, dan kan je dit op deze manier coderen, binnen het onderdeel network_settings (zie hieronder voor links naar online documentatie hierrond): memory: sequence_length: 64 memory_size: 256 6.1.4 Beloningsysteem Het einddoel van het NN is om een beleid te vinden dat de beloning zo groot mogelijk maakt. De beloningen die in Obelix.cs bepaald werden, noemt men de extrinsieke beloningen (eng: extrinsic rewards) omdat ze door de Unity Agent en niet door het leeralgoritme zelf worden bepaald. Daarnaast kan men nog een aantal intrinsieke beloningen definiëren. Uiteindelijk zal het leeralgoritme een mix van extrinsieke en intrinsieke beloningen gebruiken. reward_signals: extrinsic: strength: 1.0 gamma: 0.99 curiosity: strength: 0.02 gamma: 0.99 encoding_size: 256 learning_rate : 1e-3 curiosity is zo een intrinsieke beloning die gebruikt kan worden indien de beloningen schaars zijn en indien de kans dus klein is om beloond te worden. Door deze waarde te verhogen, verhoog je ook de kans dat de agent ‘per ongeluk’ de juiste actie uitvoert. strength geeft een gewicht aan het specifiek type belonging gamma is een vermenigvuldigingsfactor die aangeeft in hoeverre de agent moet anticiperen op toekomstige beloningen van dit type encoding_size: (optioneel) grootte van het intrinsiek model learning_rate: (optioneel) snelheid waarmee de strength mag afnemen 6.1.5 Extra informatie rond configuratie Meer informatie rond de configuratie van agents vind je hier en hier. Je kan Environment Parameters definiëren in de root van jouw configuratiebestand die je dan vanuit C# kan aanspreken (zie hier) Informatie rond algoritme families 6.2 Start de training Hier is een stappenplan voor het starten van de training: Controleer de instellingen nog eens: Zorg dat het Unity project open is en zorg ervoor dat de Behavior Type in de Behavior Parameters-component op Default staat. Navigeer naar de folder van je Unity project en maak daarin een sub-folder Learning Kopieer het yaml configuratiebestand in deze folder Open een console van je OS in deze sub-folder Indien van toepassing, activeer dan de gepaste Python omgeving Voer de Python mlagents-learn functie uit met als parameters de naam van het configuratiebestand en een naam voor de uitvoering (eng: run): mlagents-learn Obelix-01.yml --run-id Obelix-01 Krijg je het Unity logo te zien, dan zit het meestal goed. Anders moet je de foutmeldingen lezen en indien nodig opzoeken. ▄▄▄▓▓▓▓ ╓▓▓▓▓▓▓█▓▓▓▓▓ ,▄▄▄m▀▀▀&#39; ,▓▓▓▀▓▓▄ ▓▓▓ ▓▓▌ ▄▓▓▓▀&#39; ▄▓▓▀ ▓▓▓ ▄▄ ▄▄ ,▄▄ ▄▄▄▄ ,▄▄ ▄▓▓▌▄ ▄▄▄ ,▄▄ ▄▓▓▓▀ ▄▓▓▀ ▐▓▓▌ ▓▓▌ ▐▓▓ ▐▓▓▓▀▀▀▓▓▌ ▓▓▓ ▀▓▓▌▀ ^▓▓▌ ╒▓▓▌ ▄▓▓▓▓▓▄▄▄▄▄▄▄▄▓▓▓ ▓▀ ▓▓▌ ▐▓▓ ▐▓▓ ▓▓▓ ▓▓▓ ▓▓▌ ▐▓▓▄ ▓▓▌ ▀▓▓▓▓▀▀▀▀▀▀▀▀▀▀▓▓▄ ▓▓ ▓▓▌ ▐▓▓ ▐▓▓ ▓▓▓ ▓▓▓ ▓▓▌ ▐▓▓▐▓▓ ^█▓▓▓ ▀▓▓▄ ▐▓▓▌ ▓▓▓▓▄▓▓▓▓ ▐▓▓ ▓▓▓ ▓▓▓ ▓▓▓▄ ▓▓▓▓` &#39;▀▓▓▓▄ ^▓▓▓ ▓▓▓ └▀▀▀▀ ▀▀ ^▀▀ `▀▀ `▀▀ &#39;▀▀ ▐▓▓▌ ▀▀▀▀▓▄▄▄ ▓▓▓▓▓▓, ▓▓▓▓▀ `▀█▓▓▓▓▓▓▓▓▓▌ ¬`▀▀▀█▓ Je krijgt dan slechts enkele seconden om binnen de Unity GUI de ►-toets in te drukken Je kan alternatief ook werken met Unity Executables. Je kan een stopgezette trainging hervatten met de --resume parameter van mlagents-learn Voor hulp: mlagents-learn --help De training loopt zolang max_steps in het configuratiebestand niet bereikt is, maar je kan de training vroeger stoppen als je wil. Je stopt de training door nogmaals de ►-toets in te drukken. Tijdens de trainingsfase kan je het verloop van de gemiddelde geaccumuleerde beloning per episode volgen door middel van Tensorboard. Dit is het dashboard van TensorFlow dat als lokale server op jouw machine draait. Je kan Tensorboard starten dezelfde sub-folder (Learning): tensorboard --logdir results In je browser ga je dan naar http://localhost:6006 en je zou het dashboard moeten zien verschijnen. Zie § Tensorboard voor meer informatie. 6.3 Paralleliseren Om de trainingsfase te verkorten, kan je de omgevingen dupliceren. De agents binnen de gedupliceerde omgevingen moeten allen gekoppeld zijn met hetzelfde NN via eenzelfde Behavior Name eigenschap in de Behavior Parameters component. Figuur 6.1: Meerdere gedupliceerde omgevingen om de training te bespoedigen. "],
["testen-van-een-nn.html", "Hoofdstuk 7 Testen van een NN 7.1 Het nn-bestand 7.2 Tensorboard", " Hoofdstuk 7 Testen van een NN 7.1 Het nn-bestand Het belangrijkste resultaat van de trainingsfase is een *.nn-bestand dat het model van het NN bevat. Het kan als bestand ge-dropped in het daarvoor bedoelde veld van de Behavior Parameters-component. Vanaf dit ogenblik kan je van elke Obelix een slimme Obelix maken! Als je dan opnieuw het spel start zal Unity dit brein gebruiken (eng: inference) en niet langer bijleren. Denk eraan om de Behavior Type op Default of Inference Only te zetten. 7.2 Tensorboard Nog eens herhalen, je start een lokale Tensorboard server met tensorboard --logdir results. In je browser ga je naar http://localhost:6006 en je zou het dashboard moeten zien verschijnen: De belangrijkste grafiek is diegene die de evolutie van belonging weergeeft (links): De andere grafieken kunnen erg afhangen van de instellingen en gaan we voor deze cursus niet dieper op in. Bovendien vereisen ze soms extra achtergrondkennis: Wil je toch meer te weten komen over deze andere grafieken of over de andere tabs van de TensorBoard applicatie, kijk dan hier. "],
["zelf-een-eigen-intelligente-agent-creëren.html", "Hoofdstuk 8 Zelf een eigen intelligente agent creëren", " Hoofdstuk 8 Zelf een eigen intelligente agent creëren Bovenop de interacties die de agent kan hebben met de andere spelobjecten moet je over 4 punten zeer goed nadenken: Wat zijn de bewegingsvrijheden van mijn agent, i.e. de set van acties die de agent kan uitvoeren? Bijv.: {stap vooruit, stap achteruit, draai links, draai rechts} Wat zijn de verschillende statussen waarin de agent zich kan bevinden? Bijv.: {menhir op rug, geen menhir op rug} Wat kan de agent waarnemen? Bijv.: {2 × 13 ray-casts over totale hoek van 120°} Hoe zet ik het beloningssysteem op? De eerste 3 punten komen overeen met de invoer en de uitvoer van het NN. Om te helpen een juiste keuze te maken voor deze drie zaken, zijn er andere vragen die je jezelf kan stellen: Zijn de aan te leren handelingen/wijzigingen eenvoudig programmeerbaar? Zo ja, dan is het inschakelen van een NN misschien wel niet de beste oplossing! → Maak de handelingen complexer Zijn de acties verenigbaar? Soms zijn bepaalde combinaties van acties onverenigbaar. Zo zou men willen kunnen verhinderen dat Obelix draait terwijl hij achteruit gaat. → Gebruik masking, zie hier voor meer uitleg hierover. Zijn de observaties überhaupt voldoende om de nodige verborgen patronen in de invoer te ontdekken? → Voeg observaties toe Is het scenario tijdens de trainingsfase representatief voor de situaties die de agent in het finale spel (of de finale VR-simulatie) zal tegenkomen → her-analyseer de logica van het spel of de simulatie Zijn de gegevens die gebruikt worden voor het beloningssysteem ook werkelijk waarneembaar door de agent? Bijvoorbeeld, leer je een agent om een balletje met een boog in een korfbal-korf te gooien (als dusdanig slecht voorbeeld want eenvoudig programmeerbaar). Behoort de eigenlijke vorm van de boog of de precieze tijdspanne dat de bal in de lucht is tot de waarnemingen? Of enkel de richting waarin en de kracht waarmee de bal wordt geworpen? In het eerste geval geef je het antwoord a.h.w. al mee aan het NN, terwijl het net de bedoeling is dat de agent de handeling ‘uit zichzelf’ aanleert! Waar je je best niet teveel mee inlaat zijn de afwerking, de texturen, de materialen, enz. … Tenslotte, nog wat tips voor het trainen van je agent: Stelling 8.1 (Eenvoudige start) Begin zo eenvoudig mogelijk. Pas wanneer het trainen end-to-end goed lijkt te verlopen, voeg je geleidelijk aan meer complexiteit toe. Stelling 8.2 (Reproduceerbaarheid) Zorg ervoor dat in jullie finaal project de training van je agent reproduceerbaar is. "],
["bronvermelding.html", "Bronvermelding", " Bronvermelding "]
]
