[
["index.html", "VR Experience (ML-agents) ML-Agents Language License", " VR Experience (ML-agents) 34145/1928/2021/1/56David D’Haese Gepubliceerd op 2020-10-05Spirogyra sp., Motic BA310 ML-Agents In the course VR Experience the student is being submerged in VR- and 3D game development using the Unity engine. The student will learn to work with different camera perspectives, the physics engine, audio, light and scripting. In addition, using the ML-Agents toolkit, the student will learn to enrich applications with AI. The student will be able to train agents using artificial neural networks or other ML algorithms. This repository deal only with the latter part of the course, namely the the use of ML-agents as well as a basic introduction to ML. Language The course is in Dutch. License The content of this project including its source code is licensed under the GNU Affero General Public v3.0 license. You are allowed to… Under the conditions… You are not allowed to… Commercial use Disclose source Liability Distribution License and copyright notice Warranty Modification Network use is distribution Patent use Same license Private use State changes © 2020 David D’Haese "],
["inleiding-tot-het-onderdeel-ml-agents.html", "Hoofdstuk 1 Inleiding tot het onderdeel ML-Agents 1.1 In een notedop 1.2 Leerdoelen 1.3 Cursus vorm 1.4 Licentie voor deze cursus 1.5 Verwijzen naar deze cursus", " Hoofdstuk 1 Inleiding tot het onderdeel ML-Agents 1.1 In een notedop In dit opleidingsonderdeel ga je in teamverband aan de slag met de Unity game engine en leer je de 3D-toepassingen ontwikkelen. Deze 3D-toepassingen gaan van games tot interactieve Virtual Reality applicaties. Je leert werken met verschillende camera perspectieven, de physics engine, geluid, licht en scripts. Met ML-Agents Toolkit leer je hoe je met artificiële intelligentie een virtuele omgeving en zijn agents kunt laten aanpassen aan impulsen van buitenaf. Dit archief biedt hulp bij het gebruik van ML-agents en geeft ook een algemene introductie tot ML. 1.2 Leerdoelen Hieronder, in Tabel 1.1, staan de leerdoelen opgesomd zoals ze vermeld staan in de ECTS fiches die van toepassing zijn voor dit onderdeel van het OLOD VR Experience. In de cursus zal er naar deze leerdoelen verwezen worden met vermelding van de bijhorende code. Tabel 1.1: Leerdoelen voor deze cursus Code Omschrijving TI_LD568 Begrijpt de principes van artificiële intelligentie TI_LD569 Ontwikkelt een AI-module dat een cruciaal onderdeel vormt van een Unity project TI_LD570 Gebruikt en kent specifieke ML bibliotheken TI_LD571 Traint, test en evalueert een machine learning algoritme volgens de regels van de kunst 1.3 Cursus vorm Deze cursus is geschreven in een versie van Markdown. Markdown is een familie van zogenaamde opmaaktalen (eng: mark-up languages) die ervoor zorgen dat inhoud van het document in verscheidene formaten weergegeven kan worden: PDF, HTML, …. Het loskoppelen van inhoud betekent enerzijds dat de auteur zich kan focusseren op de inhoud in plaats van de vorm. Anderzijds betekent het dat de lezer in staat is zijn de vorm van de uitvoer te bepalen, bijvoorbeeld, beter leesbaarheid, donkere achtergrond, …. Voor meer technische documenten biedt Markdown nog veel belangrijkere voordelen. Het maakt het mogelijk om code in de ene of andere taal tussen de lyrische tekst te plaatsen en uit te voeren. Met de juiste IDE (Integrated Development Environment), betekent dit dat de auteur én de lezer in staat zijn om in meerdere programmeertalen tegelijkertijd te werken! Figuur 1.1: Werking van Markdown. De platte tekst (links) wordt omgezet naar een ander formaat (rechts; hier HTML) door een externe tool als Pandoc. Stijl-regels worden hier automatisch uitgevoerd maar de auteur heeft de mogelijkheid ook deze in detail te configureren. Naast het scheiden van vormgeving en inhoud (hetgeen een merkelijke efficiëntie verbetering met zich meebrengt) ondersteund R Markdown ook meertaligheid, i.e. meerdere programmeertalen in één document. Tussen een aantal talen is er zelfs wederzijdse uitwisseling mogelijk van de actieve variabelen (zie oranje markeringen met pijl). Het voorbeeld met de Mandelbrot fractaal is afkomstig van Li (2017), waarvan de GitHub repository een bondige beschrijving geeft van de Mandelbrot verzameling (eng: Mandelbrot set) met een test die de performantie-winst van Julia t.o.v. R onderzoekt. 1.4 Licentie voor deze cursus De inhoud van deze cursus valt onder een GNU Affero General Public v3.0 licentie. Wat er toegelaten is en onder welke voorwaarden staat hieronder opgesomd: Je mag… Onder voorwaarde dat… Je mag niet… Commercieel gebruik Ontsluit bron Aansprakelijk stellen Verspreiden Licentie en copyright notice mee verspreiden Garantie Aanpassen Netwerk verspreiding Patenteren Zelfde licentie Privé gebruiken Bekendmaking wijzigingen 1.5 Verwijzen naar deze cursus In Bibtex-formaat: @online{dhaese2020ml-agents, author = {D’Haese, David}, title = “VR Experience (ML-agents)”, year = “2020”, url = “https://ddhaese.github.io/vr-experience/”, note = \"[Online; accessed } APA-formaat: D’Haese, D., 2020. VR-Experience (ML-Agents) [WWW Document] [Online; accessed yyyy-mm-dd]. URL https://ddhaese.github.io/vr-experience/ © 2020 David D’Haese Ontwikkeling van een brein "],
["beginselen-ml-agents.html", "Hoofdstuk 2 Beginselen ML-Agents 2.1 Vereiste voorkennis en voorinstallatie 2.2 Installatie 2.3 ML-Agents Toolkit 2.4 Machine learning in een notedop 2.5 Obelix de menhirhouwer 2.6 Het spelverloop 2.7 Observaties, acties en beloning 2.8 Het speelveld 2.9 Spelobjecten", " Hoofdstuk 2 Beginselen ML-Agents 2.1 Vereiste voorkennis en voorinstallatie Om onderstaande handleiding te kunnen volgen moet je eerst op Digitap de volgende modules hebben afgewerkt: Week 1 - Introduction Virtual Reality Week 2 - Introduction &amp; Unity Basics Week 3 - Unity - Materials &amp; lighting &amp; audio, camera &amp; deployment Week 4 - Physics , Player control &amp; UI Week 5 - Shooter Game De vereiste software voor dit onderdeel is Unity versie 2019.4.10. 2.2 Installatie Opgelet: De versie van de geïnstalleerde software speelt hier een belangrijke rol. ML-Agents is nog volop in ontwikkeling en de functionaliteiten kunnen tussen twee versies erg verschillen. Release 6 van het ML-Agents pakket vind je op de ML_Agents GitHub repository. Gedetailleerde installatie instructies vind je op deze handleiding, maar niet alle stappen zijn noodzakelijk. Hieronder volgt de verkorte versie: Installeer Python Wil je met een omgeving-manager werken zodat je in een afgezonderde omgeving kunt werken met zijn eigen specifieke set aan Python modules, dan kan je Anaconda of de vereenvoudigde versie Miniconda downloaden in een folder waarvan het pad geen spaties bevat. Je test deze installatie met het commando conda info. Je kan een ontwikkel omgeving (eng: environment) maken met het commando conda create -n ml-agents en deze omgeving activeren met conda activate ml-agents en deactiveren met conda deactivate. Met de ml-agents omgeving actief kan je met de opdracht python --version controleren dat de versie van Python voldoende recent is (3.8.1 of hoger) en met pip install mlagents installeer je de mlagents Python module Indien de geen omgeving-manager wenst te gebruiken of de installatie hiervan werkt niet goed op jouw machine, installeer dan gewoonweg de nieuwste versies van Python en pip en voer de opdracht pip install mlagents uit aan de prompt. (Optioneel, maar sterk aangeraden) Om toegang te hebben tot een aantal eenvoudige voorbeelden kopieer je best de repository met volgend git commando: git clone --branch release_6 https://github.com/Unity-Technologies/ml-agents.git 2.3 ML-Agents Toolkit De Unity Machine Learning Agents Toolkit (of kortweg ML-Agents) stelt de spel- of simulatie-ontwikkelaar in staat om één of meerdere karakters (eng: non-player characters of NPCs) of agenten (eng: agents) intelligent te maken door deze een virtueel brein mee te geven. Je bepaalt zelf wat je de agenten aanleert en dus wat er in het brein terechtkomt. Als het brein voldoende complex is en als er voldoende leertijd voorzien wordt zijn er bijna geen limieten aan wat je een agent kunt aanleren. Flexible Training Scenarios While the discussion so-far has mostly focused on training a single agent, with ML-Agents, several training scenarios are possible. We are excited to see what kinds of novel and fun environments the community creates. For those new to training intelligent agents, below are a few examples that can serve as inspiration: Hier is een voorbeeld: De toepassingen zijn legio: Single-Agent: Je traint één agent om iets uit zichzelf te leren op basis van observaties die het doet (zoals de overstekende kip uit de video hierboven). Simultaneous Single-Agent: Meerdere onafhankelijke karakters gekoppeld aan eenzelfde brein. Adversarial Self-Play: Meerdere karakters met het zelfde brein maar tegenovergesteld beloningssysteem zodat de agent van zichzelf leert. Deze strategie is toegepast bij AlphaGo Cooperative Multi-Agent: Meerdere agent die leren samenwerken doordat ze een beloningssysteem delen. Competitive Multi-Agent: Meerdere teams van interagerende agenten. Het soccer voorbeeld illustreert deze strategie. Ecosystem: Complex systeem met meerdere agenten elke met onafhankelijke beloningssystemen. De onderstaande figuur biedt een overzicht van de elementen in de ML-Agents toolkit. Figuur 2.1: Overzicht van de elementen van de ML-Agents toolkit. De Academy zorgt ervoor dat de agent gesynchroniseerd zijn en beheert de nodige omgeving-brede instellingen. Gebaseerd op bron. 2.4 Machine learning in een notedop Machine learning (ML) is het vermogen van een algoritme om te leren uit data. Er zijn verscheidene vormen, maar het komt er meestal op neer dat op basis van data een model wordt gemaakt. De vorm van ML waar we hier mee in contact komen is conditionering (eng: reinforcement learning) Zulk een conditionering-model kan men beschouwen als een functie dat op basis van een bepaalde status de volgende actie probeert te voorspellen. Bijvoorbeeld, het model moet aanleren dat bij de input: …er is een wagen rechts van me die met hoge snelheid nadert… een gepaste actie zou kunnen zijn: Zet snel twee stappen naar voren Bij de input: …er staat een boom vlak voor mijn neus… zou dan weer de volgende actie passen: Zet een stap opzij. De set van alle combinaties van een status met de voorgestelde actie noemt men de aangeleerd beleid (eng: policy). Stelling 2.1 Machine learning (ML) is het vermogen van een algoritme om te leren uit data. Figuur 2.2: Overzicht proces voor conditionering. Een toestand ruimte wordt gedefinieerd of er wordt een functie voorzien die kan meegeven of een toestand mogelijk is of niet. Daarnaast moet er een beloning systeem worden meegegeven zodat de tolk voor elke mogelijke handeling een beloning kan bepalen (of bestraffing door middel van een negatieve beloning). Figuur 2.3: Een detail van het conditioneringsproces. Het model wordt in dit geval door Python afgehandeld en er is een Python Low-Level API om te communiceren tussen Python en de Unity Academy. 2.5 Obelix de menhirhouwer Vele mensen kennen de verhalen van Asterix en Obelix. Hierin speelt Obelix de rol van een menhirhouwer met bovennatuurlijke krachten. Spijtig genoeg is hij nogal naïef en doet hij wel eens domme dingen. Laten daar verandering in brengen door Obelix een brein te geven! Figuur 2.4: Kunnen we Obelix de menhirhouwer een tikkeltje slimmer maken? bron afbeelding van Obelix. 2.6 Het spelverloop Het spelverloop wordt hieronder schematisch voorgesteld. Obelix is het hoofdpersonage en moet op zoek naar vrije menhirs. Aangekomen bij een menhir moet hij deze op zijn rug nemen en op zoek gaat naar een bestemming. Figuur 2.5: Het spelverloop hier uitgebeeld voor een enkele vrije, stilstaande menhir. 2.7 Observaties, acties en beloning Er zijn drie zaken dat je bij een conditionering altijd eerst rustig over moet nadenken alvorens als een gek te beginnen ontwikkelen of ontwerpen: observaties, acties en beloning. Stelling 2.2 (Observaties) Observaties die de agent maakt op basis van de omgeving vormen de invoer voor het leeralgoritme. In het geval van Obelix zijn de observaties diegenen die hij maakt door rond te kijken. We gaan zien dat ML-Agents ons hierbij helpt door componenten aan te bieden voor visuele perceptie. Stelling 2.3 (Acties) Acties bepalen de bewegingsvrijheid van een agent of, algemener gesteld, de impact die een agent heeft op zijn omgeving. Zolang de agent niet veranderd in geen enkel van zijn eigenschappen (positie, snelheid, kleur, tags,…) dan kan het leeralgoritme daar niets mee, want wat het ook voorstelt als te ondernemen actie, er gebeurt toch niets. Voor Obelix is het al duidelijk geworden dat hij moet kunnen rondkijken om ervoor te zorgen dat er een menhir of bestemming in het gezichtsveld treedt. Verder moet hij kunnen ‘rondlopen’ zodat hij een menhir of bestemming kan benaderen. Als we er van uitgaan dat het plaatsen van de menhir op de rug en het daarna het lossen van de menhir op een bestemming volautomatisch gebeurt (i.e. hij moet er gewoon maar mee botsen) dan zijn er geen verdere acties die Obelix moeten kunnen ondernemen. De details van het roteren en het rondlopen bespreken we later. Stelling 2.4 (Beloning) Het beloningsmechanisme (eng: rewards, incentives) vertelt het leeralgoritme of de voorgestelde actie de agent dichter bij het einddoel van de leeroefening brengt of niet. Figuur 2.6 geeft een overzicht van het beloningssysteem voor het Obelix spel. Het einddoel is het plaatsen van de menhir op een bestemming en dus daar krijgt het leeralgoritme de grootste score voor. Voor het naderen van een menhir of een bestemming krijgt de agent enkel een (matige) beloning indien het zich in de juiste staat bevindt. Bijvoorbeeld, als Obelix al een steen op zijn rug draagt, krijgt hij geen extra punten om een andere menhir te benaderen, want twee menhirs op de rug is misschien toch wat veel van het goede (in deze versie, natuurlijk kan je dit aanpassen). Figuur 2.6: Het beloningssysteem voor het Obelix-spel. 2.8 Het speelveld Laten we beginnen met alle componenten te bouwen om dit spel te bouwen. Merk op dat het verstandig is om eerst de ML logica in te bouwen alvorens je de spelobjecten tot in het detail uitwerkt. Dat is ook wat we hier gaan doen. Figuur 2.7: Het speelveld met 5 spelobjecten. 2.9 Spelobjecten 2.9.1 Speelveld object Het speelveld krijgt de naam Hange en is een eenvoudig vlak met schaal X = Y = Z = 2 en posities en rotatie op X = Y = Z = 0 en met een Mesh Collider. Dit en alle andere objecten zitten vervat in een container object Trainer zodat het geheel later eenvoudig gedupliceerd kan worden. Figuur 2.8: De volledige hiërarchie binnen de spelobjecten met hun namen zoals ze in deze handleiding gebruikt zullen worden. Merk op dat de Menhir en Destination objecten prefabs zijn zodat men hier meerdere van kan genereren (eng: to spawn) tijdens de initialisatie van het spel. 2.9.2 Obelix spelobject Figuur 2.9: Obelix spelobject. Zie tekst voor de eigenschappen van dit object. Dit is het enige composiet spelobject met het cilindrisch lichaam Obelix, een rechteroog (RightEye), een linkeroog (LeftEye) en een neus (Nose). Het lichaam zelf is een capsule met schaal 1 in alle richtingen. Het bevat een passende Capsule collider en een Rigid body component met de rotatie-vrijheid beperkt tot enkel de y-as (dus freeze rotation X en Z aangevinkt). De ogen en de neus worden als dochter-componenten van het lichaam aangemaakt en bestaan uit bollen, respectievelijk een capsule, telkens met hun gepaste en aansluitende collider. Zorg dat het composiet object voldoende boven de grond is (y = 1.5) zodat hij netjes op het speelveld valt. De acties die Obelix gaat doen zijn gebaseerd op observaties. Zorg er dan ook voor dat beide ogen van Obelix kunnen ‘zien’ door aan beide ogen een Ray Perception Sensor 3D toe te voegen met de volgende eigenschappen (hier voor het links oog weergegeven). Eigenschap Waarde Uitleg Sensor Name LeftEye Unieke naam is belangrijk Detecable Tags Menhir en Destination De objecten die Obelix kan zien Rays Per Direction 6 Aantal stralen aan weerszijden van middellijn. Max Ray Degrees 30 Voor een zicht van 60° per oog. Sphere Cast Radius 3.5 De dikte van elke straal Ray Length 260 Lengte van de stralen Stacked Raycasts 1 Geheugen voor de sensor Start/End Vertical Offset 0/0 Naar boven of naar onder kijken Obelix krijgt dus een zicht van 120°. Hij zal zich dus moeten draaien om een volledig beeld te krijgen van zijn omgeving. De lengte van de stralen is zodanig gekozen dat hij van de ene hoek van het speelveld nog net een object in de tegenoverstaande hoek kan zien. Doe hetzelfde voor het rechteroog (je kan de component eenvoudig kopiëren). Roteer nu de ogen +/-30° volgens de Y-as zoals getoond op Figuur 2.10 zodat er een zo breed mogelijk gezicht veld benut wordt (dus geen overlap). Figuur 2.10: De Ray Perception Sensor 3D component in actie. 2.9.3 De menhir en de bestemming De menhir en de bestemming prefabs worden eenvoudig geconstrueerd als kubussen met schaal {XYZ: 0.5, 2, 0.5} met een passende Box Collider. De verschillen tussen de Menhir prefab en de Destination prefab staan hieronder opgesomd: Figuur 2.11: De menhir bestaat uit een eenvoudige balk. Eigenschap Menhir Destination Rigidbody component Use Gravity aan en Freeze Rotation voor de assen X en Y Geen Rigidbody component. Startpositie (Y) Boven het veld, valt bij opstart tot op speelveld Rustend op het speelveld Tag Menhir Destination Je kan ze natuurlijk best ook met een verschillend materiaal bekleden zodat ze gemakkelijk te onderscheiden zijn. 2.9.4 Scorebord Dit object met naam ScoreBoard is een instantie van de TextMeshPro klasse dat de gecumuleerde beloning (eng: cumulative reward) zal weergeven. Hier wijzig je de eigenschappen naar keuze. Het is weliswaar handig als je de score kan lezen vanuit een hoog perspectief. "],
["ontwikkeling-van-een-brein.html", "Hoofdstuk 3 Ontwikkeling van een brein", " Hoofdstuk 3 Ontwikkeling van een brein "]
]
